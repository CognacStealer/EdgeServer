# ğŸš€ EdgeServer â€” Local LLM Inference Server

**Author:** [@CognacStealer](https://github.com/CognacStealer)
**Version:** 1.0.0

EdgeServer is a lightweight **local LLM server** designed to run **TinyLlama (or similar small models)** on **low-end CPUs** efficiently.
It exposes **FastAPI endpoints** for inference and notebook-based fine-tuning, allowing **local model customization** without needing a high-end GPU.

---

## ğŸ§© Overview

**Core Idea:**

> EdgeServer allows any user â€” even with low-end hardware â€” to interact with and fine-tune small LLMs locally through a centralized server running entirely on CPU.
> It serves as a **bridge** that lets local clients (e.g., web apps, notebooks) use and fine-tune models efficiently via REST API requests.

### âœ¨ Features

* âœ… **CPU-optimized** â€” no GPU required
* âœ… **FastAPI backend** with CORS support
* âœ… Modular route system (`inference`, `notebook`)
* âœ… Preloaded **TinyLlama** (1.1B Chat) model
* âœ… **Deterministic, factual** output generation
* âœ… **Frontend-ready** (React / Vite compatible)

---

## ğŸ“ Project Structure

```
EdgeServer/
â”‚
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ routes/
â”‚   â”‚   â”œâ”€â”€ inference.py
â”‚   â”‚   â”œâ”€â”€ notebook.py
â”‚   â”œâ”€â”€ __init__.py
â”‚
â”œâ”€â”€ model.py                 # Model loader (TinyLlama)
â”œâ”€â”€ main.py                  # FastAPI server entry point
â”œâ”€â”€ requirements.txt         # All dependencies
â””â”€â”€ README.md                # You're reading this ğŸ™‚
```

---

## âš™ï¸ Setup Instructions

Follow these steps to set up and run **EdgeServer** locally.

---

### 1ï¸âƒ£ Clone the Repository

```bash
git clone https://github.com/CognacStealer/EdgeServer.git
cd EdgeServer
```

---

### 2ï¸âƒ£ Create a Virtual Environment

Creating a **virtual environment** isolates dependencies and ensures stability.

#### ğŸªŸ On Windows:

```bash
python -m venv venv
venv\Scripts\activate
```

#### ğŸ§ On macOS/Linux:

```bash
python3 -m venv venv
source venv/bin/activate
```

---

### 3ï¸âƒ£ Upgrade pip

```bash
pip install --upgrade pip
```

---

### 4ï¸âƒ£ Install Dependencies

Create a `requirements.txt` file if not already present, with the following content:

```txt
fastapi
uvicorn
transformers
torch
sentencepiece
```

Then install:

```bash
pip install -r requirements.txt
```

---

### 5ï¸âƒ£ Run the Server

Start your FastAPI app using **Uvicorn**:

```bash
uvicorn main:app --reload
```

**Expected Output:**

```
INFO:     Uvicorn running on http://127.0.0.1:8000
INFO:     Application startup complete.
```

---

## ğŸŒ API Endpoints

| Endpoint    | Method | Description                                      |
| ----------- | ------ | ------------------------------------------------ |
| `/`         | GET    | Root route â€” status and available endpoints      |
| `/predict`  | POST   | Generate LLM response from user prompt           |
| `/models`   | GET    | List available models (TinyLlama, Falcon, Phi-2) |
| `/run_cell` | POST   | Execute notebook-style fine-tuning code          |

---

## ğŸ§  Example Usage

### 1ï¸âƒ£ Test Root Endpoint

After the server is running:

```bash
curl http://127.0.0.1:8000/
```

Response:

```json
{
  "status": "ok",
  "message": "Edge LLM Server is running!",
  "endpoints": ["/predict", "/models", "/run_cell"]
}
```

---

### 2ï¸âƒ£ Generate LLM Response

Example query:

```bash
curl -X POST "http://127.0.0.1:8000/predict" \
-H "Content-Type: application/json" \
-d '{"prompt": "Explain edge computing in one line"}'
```

Response:

```json
{
  "response": "Edge computing processes data closer to its source to reduce latency and bandwidth."
}
```

---

## ğŸ§© Model Configuration

Default model:
`TinyLlama/TinyLlama-1.1B-Chat-v1.0`

### Available Options

| Model Name                           | Description                              |
| ------------------------------------ | ---------------------------------------- |
| `TinyLlama/TinyLlama-1.1B-Chat-v1.0` | âš¡ Best lightweight factual model for CPU |
| `tiiuae/falcon-1b-instruct`          | ğŸ¦… Slightly heavier but capable          |
| `microsoft/phi-2`                    | ğŸ§© More factual, slower on CPU           |

### Behavior Controls (in `model.py`)

* `temperature = 0.2` â†’ factual responses
* `do_sample = False` â†’ deterministic output
* `repetition_penalty = 1.1` â†’ prevents looping
* `SYSTEM_PROMPT` defines assistant personality

---

## ğŸ§  Project Goals

EdgeServer acts as a **local AI node** that:

* Helps **low-end systems** perform model inference or fine-tuning
* Allows **LAN or shared access** to local LLMs
* Enables **fine-tuning within notebooks** for domain-specific use cases

---

## ğŸ§° Developer Notes

* Runs fully on **CPU** (torch float32 + `low_cpu_mem_usage=True`)
* Extend routes easily inside `/app/routes`
* Can be deployed on **LAN** for multiple clients
* Ideal for **edge AI** and **lightweight inference** setups

---

## ğŸ§¾ License

This project is released under the **MIT License**.
You are free to use, modify, and distribute it with attribution.

---

## ğŸ‘¨â€ğŸ’» Author

**EdgeServer**
Developed & maintained by **[@CognacStealer](https://github.com/CognacStealer)**
Contributions and improvements are always welcome!

---

Would you like me to add a **Docker deployment section** (so others can run this instantly using one command like `docker run cognacstealer/edgeserver`)? Itâ€™ll make this project plug-and-play for LAN and Raspberry Pi environments.
